{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N5RuFIhveel"
      },
      "source": [
        "# Homework assignment 1: Markov Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1UaCc1Qveen"
      },
      "source": [
        "## Problem 1\n",
        "Read the article in https://www.geeksforgeeks.org/markov-chains-in-nlp/, and answer the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBof0iUZveeo"
      },
      "source": [
        "* (a) What are N-grams of an input sequence?\n",
        "    * N-grams are a contiguous sequence of n items(words or characters) from a given sample of text.\n",
        "    * For example, the input sequence \"bong likes soccer\", this is a 3-gram input.\n",
        "    * N-grams in the example above are a contiguous sequences of n words, n is usually 2 or 3.\n",
        "\n",
        "* (b) How to determine the probability of each element in the transition matrix?\n",
        "    * By counting the number of times a particular element appears after another element in the N-grams.\n",
        "\n",
        "* (c) If you want to increase the variety of the sequence generation (every time the outputs are different), what kinds of properties the training texts should be?\n",
        "    * Increase the input length: This enables the output to have more variety, as it has multiple starting state points. This also increases the transistion probabilities.\n",
        "    * Use multiple topics in text: This is done to diversify the text context and output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW3fVQRdveeo"
      },
      "source": [
        "## Problem 2\n",
        "Try the following codes, and answer questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9s0dxWdM_VHB"
      },
      "outputs": [],
      "source": [
        "# Install the required packages\n",
        "# !pip install nltk\n",
        "# !pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3rvAjDpaveep"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk import ngrams\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Union, Dict\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fuUPhTVWveeq"
      },
      "outputs": [],
      "source": [
        "def words_to_index(words: List[str], base: int, unique_words: Dict[str, int]) -> int:\n",
        "    \"\"\"\n",
        "        Convert a list of words to an index representation.\n",
        "\n",
        "        Args:\n",
        "            words (List[str]): The list of words to be converted.\n",
        "            base (int): The base value used for conversion.\n",
        "            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n",
        "\n",
        "        Returns:\n",
        "            - int: The index representation of the given list of words.\n",
        "\n",
        "        Example:\n",
        "            >>> words_to_index([\"the\", \"quick\", \"brown\", \"fox\"], 10, {\"the\": 0, \"quick\": 1, \"brown\": 2, \"fox\": 3})\n",
        "            123\n",
        "    \"\"\"\n",
        "    length = len(words)\n",
        "    numbers = [unique_words[word] for word in words]\n",
        "    return sum([num * (base ** (length - 1 - idx)) for idx, num in enumerate(numbers)])\n",
        "\n",
        "def index_to_words(index: int, base: int, length: int, unique_words: Dict[str, int]):\n",
        "    \"\"\"\n",
        "        Converts an index to a list of words based on a given base and unique words.\n",
        "\n",
        "        Args:\n",
        "            index (int): The index to convert.\n",
        "            base (int): The base used for conversion.\n",
        "            length (int): The length of the resulting list.\n",
        "            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: A list of words corresponding to the given index.\n",
        "\n",
        "        Examples:\n",
        "            >>> unique_words = {'apple': 0, 'banana': 1, 'cherry': 2}\n",
        "            >>> index_to_words(5, 3, 2, unique_words)\n",
        "            ['banana', 'cherry'] (Because 5 = 1 * 3^1 + 2 * 2^0)\n",
        "\n",
        "            >>> unique_words = {'red': 0, 'green': 1, 'blue': 2}\n",
        "            >>> index_to_words(2, 3, 1, unique_words)\n",
        "            ['blue'] (Because 2 = 2 * 3^0)\n",
        "    \"\"\"\n",
        "    numbers = []\n",
        "\n",
        "    unique_words_list = list(unique_words.keys())\n",
        "    for pow in range(length - 1, -1, -1):\n",
        "        numbers.append(index // (base ** pow))\n",
        "        index -= numbers[-1] * (base ** pow)\n",
        "\n",
        "    return [unique_words_list[num] for num in numbers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q89RN21veer"
      },
      "source": [
        "### Step 1: Remove some unnecessary characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-bLwYWvDveer"
      },
      "outputs": [],
      "source": [
        "def remove_unnecessary_characters(text: str):\n",
        "    \"\"\"\n",
        "        Removes unnecessary characters from the given text and converts it to lowercase.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to be processed.\n",
        "        Returns:\n",
        "            str: The processed text with unnecessary characters removed and converted to lowercase.\n",
        "    \"\"\"\n",
        "    # Remove unnecessary characters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDPRDP_Dveer"
      },
      "source": [
        "### Step 2: Generate N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_XqvOwjOvees"
      },
      "outputs": [],
      "source": [
        "def generate_n_grams(text: str, n: int):\n",
        "    \"\"\"\n",
        "        Generate n-grams from the given text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which n-grams will be generated.\n",
        "            n (int): The number of consecutive words in each n-gram.\n",
        "        Returns:\n",
        "            List[Tuple[str]]: A list of tuples representing the generated n-grams.\n",
        "    \"\"\"\n",
        "    # [TODO] Generate n-grams, using the ngrams function from the nltk library\n",
        "    n_grams = ngrams(text.split(), n)\n",
        "\n",
        "    # Convert to list of tuples\n",
        "    n_grams = list(n_grams)\n",
        "\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLrCGQuyvees"
      },
      "source": [
        "### Step 3: Compute Transition Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ij_Pxsn6vees"
      },
      "outputs": [],
      "source": [
        "def compute_transition_matrix(n_grams: List[Tuple[str]]):\n",
        "    \"\"\"\n",
        "        Computes the transition matrix and unique states for a given list of n-grams.\n",
        "\n",
        "        Args:\n",
        "            n_grams (List[Tuple[str]]): A list of n-grams, where each n-gram is a tuple of strings.\n",
        "        Returns:\n",
        "            np.ndarray: transition matrix.\n",
        "            Dict[str, int]: A dictionary mapping unique words to their corresponding indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the value of n\n",
        "    n = len(n_grams[0])\n",
        "\n",
        "    ## Step 3.1: Collect all possible words (label words with indices)\n",
        "\n",
        "    unique_words = {}\n",
        "    unique_words_count = 0\n",
        "\n",
        "    # A helper function to add a word to the unique_words dictionary, if it is not already present\n",
        "    def _add_to_unique_word(word: str):\n",
        "        # We need to increase the counter outside this function\n",
        "        nonlocal unique_words_count\n",
        "\n",
        "        # Check if the state is already in the unique_states dictionary\n",
        "        if unique_words.get(word, None) is None:\n",
        "            unique_words[word] = unique_words_count\n",
        "            unique_words_count += 1\n",
        "\n",
        "    # Iterate over all n-grams to collect all possible words\n",
        "    for n_gram in n_grams:\n",
        "        for word in n_gram:\n",
        "            # [TODO] Add the word to the unique_words dictionary\n",
        "            # hint: use the _add_to_unique_word function\n",
        "            _add_to_unique_word(word)\n",
        "            continue\n",
        "\n",
        "\n",
        "    ## Step 3.2: Compute the transition matrix\n",
        "\n",
        "    # In this part, we use `words_to_index` and `index_to_words` functions to convert between state name (words) and indices\n",
        "\n",
        "    # Example:\n",
        "    # If we have 3 unique words: 0 for \"apple\", 1 for \"banana\", 2 for \"cherry\", and n = 3,\n",
        "    # then the state names are \"apple, apple\", \"apple, banana\", \"apple, cherry\", \"banana, apple\", \"banana, banana\", and so on.\n",
        "    # In this case, the mapping from states to indices would be:\n",
        "    # \"(apple, apple)\" <-> 0, \"(apple, banana)\" <-> 1, \"(apple, cherry)\" <-> 2, \"(banana, apple)\" <-> 3, and so on.\n",
        "\n",
        "    # [TODO] Compute the number of states in Markov chain. Number of states is the number of unique words to the power of n-1\n",
        "    unique_states_count = unique_words_count ** (n-1)\n",
        "    # [TODO] Create transition matrix, filled with zeros (the size of the transition matrix is # states x # states)\n",
        "    transition_matrix = np.zeros((unique_states_count,unique_states_count))# pokoknya di isi kosong semua berdasarkan jumlah states\n",
        "\n",
        "    # Count the number of transitions from each state to another state\n",
        "    for n_gram in n_grams:\n",
        "        # [TODO] Find the indices of the states (from and to) (i.e. the first n-1 words and the last n-1 words)\n",
        "        # hint: use words_to_index function\n",
        "        state_from_index = words_to_index(n_gram[:-1], unique_words_count, unique_words) #mulai dari elemen plg awal\n",
        "        state_to_index = words_to_index(n_gram[1:], unique_words_count, unique_words) #sampe berakhir di elemen terakhir\n",
        "\n",
        "        # Increment the count of the transition from state_from to state_to\n",
        "        transition_matrix[state_from_index][state_to_index] += 1\n",
        "\n",
        "    # Special case:\n",
        "    # if the sum of number of transition of a state is zero, which means it has no information about the next state,\n",
        "    # we set the probabilities of transitioning to all possible next states to 1 / # words\n",
        "\n",
        "    # Check if any row sums to zero\n",
        "    row_sums_is_zero = transition_matrix.sum(axis=1) == 0\n",
        "\n",
        "    # Iterate over rows to find zero-sum rows\n",
        "    first_word = list(unique_words.keys())[0]\n",
        "    for idx, is_zero in enumerate(row_sums_is_zero):\n",
        "        # If the row sum is zero\n",
        "        if is_zero:\n",
        "            # Convert the index to state representation\n",
        "            state_name = index_to_words(idx, unique_words_count, n-1, unique_words)\n",
        "\n",
        "            # Calculate the range of indices for the next posiible states\n",
        "            possible_states_start_index = words_to_index(state_name[1:] + [first_word, ], unique_words_count, unique_words)\n",
        "            possible_states_end_index = possible_states_start_index + unique_words_count\n",
        "\n",
        "            # Set the number of transitions to 1 for the range\n",
        "            transition_matrix[idx, possible_states_start_index : possible_states_end_index] = 1\n",
        "\n",
        "    ## Step 3.3: Normalize the transition matrix\n",
        "\n",
        "    # The following steps is to normalize transition matrix so that the sum of each row is 1\n",
        "    # (The reason is: By the definition of a Markov chain, the sum of each row should be 1)\n",
        "\n",
        "    # [TODO] Compute the sum of each row\n",
        "    # hint: use np.sum function and check if the axis parameter is set correctly\n",
        "    row_sums = np.sum(transition_matrix, axis=1)# karena nyari sum of each row pake 1, kalo nyari sum of each column baru pake 0\n",
        "\n",
        "    # [TODO] Divide the transition matrix by the sum of each row to make the sum of each row equals to 1\n",
        "    transition_matrix = transition_matrix / row_sums[:, np.newaxis] # intinya dibagi sum of each row biar sum of rownya jadi 1\n",
        "\n",
        "    return transition_matrix, unique_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLiaaGcpveet"
      },
      "source": [
        "### Step 4: Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bh7NXbk4veet"
      },
      "outputs": [],
      "source": [
        "def generate(unique_words: Dict[str, int], transition_matrix: np.ndarray, start_from: Union[str, List[str]], n: int, length: int=10):\n",
        "    \"\"\"\n",
        "        Generate text using a Markov chain model.\n",
        "\n",
        "        Args:\n",
        "            unique_states (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n",
        "            transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n",
        "            start_from (Union[str, List[str]]): The starting state(s) for text generation.\n",
        "            n (int): The size of the grams.\n",
        "            length (int, optional): The length of the generated text. Defaults to 10.\n",
        "\n",
        "        Returns:\n",
        "            The generated text.\n",
        "    \"\"\"\n",
        "    # Generate text\n",
        "    generated_words = start_from.copy() if type(start_from) is list else [start_from]\n",
        "\n",
        "    # Assert if the number of start words does not equal to n-1\n",
        "    assert len(generated_words) >= n-1, \"The number of start words should be greater than or equals to n-1 ({})\".format(n-1)\n",
        "\n",
        "    # Get the number of unique words\n",
        "    unique_words_count = len(unique_words)\n",
        "\n",
        "    # [TODO] Get the number of unique states\n",
        "    # hint: check step 3.2 in the compute_transition_matrix function\n",
        "    unique_states_count = unique_words_count ** (n-1)\n",
        "\n",
        "    # Generate the next words\n",
        "    for _ in range(length):\n",
        "        # [TODO] Get index of current states\n",
        "        # hint: The current states (current words) is the last n-1 words in the generated text\n",
        "        # hint: use words_to_index function\n",
        "        current_words_index = words_to_index(generated_words[-n+1:], unique_words_count, unique_words)\n",
        "\n",
        "        # [TODO] Get probability distribution for next state, using the information in the transition matrix\n",
        "        probabilities = transition_matrix[current_words_index]\n",
        "\n",
        "        # Select next word based on probabilities, using np.random.choice function\n",
        "        next_words_index = np.random.choice(unique_states_count, p=probabilities)\n",
        "\n",
        "        # [TODO] Decode the index and get the last word\n",
        "        # hint: use index_to_words function\n",
        "        next_word = index_to_words(next_words_index, unique_words_count, n-1, unique_words)[-1]\n",
        "        # because we use the first 2 words to predict the next word,\n",
        "        # therefore when the index is converted back to word, it is n-1 length bcs we only use the first n-1 words in the list\n",
        "\n",
        "        # Add next word to generated text\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "    # return generated string\n",
        "    return ' '.join(generated_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KkOugSjwveeu"
      },
      "outputs": [],
      "source": [
        "# [TODO] Change the text below and try different values of n\n",
        "#text = \"i love cat. cat love me. But I love dog too and dog also love me\"\n",
        "text=\"kevin lius bong. icho 3x winner. chemical engineering nus. hobi dia futsal. dia suka makan mie. dia suka minum matcha.\"\n",
        "n = 4\n",
        "\n",
        "# Process the text and generate the transition matrix\n",
        "text = remove_unnecessary_characters(text)\n",
        "n_grams = generate_n_grams(text, n)\n",
        "transition_matrix, unique_words = compute_transition_matrix(n_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4MAL08Egveeu",
        "outputId": "abfc6a15-43b1-4ef3-98e7-6e43a4926323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== The indices for unique states are: =====\n",
            "kevin,kevin,kevin: 0\n",
            "kevin,kevin,lius: 1\n",
            "kevin,kevin,bong: 2\n",
            "kevin,kevin,icho: 3\n",
            "kevin,kevin,3x: 4\n",
            "kevin,kevin,winner: 5\n",
            "kevin,kevin,chemical: 6\n",
            "kevin,kevin,engineering: 7\n",
            "kevin,kevin,nus: 8\n",
            "kevin,kevin,hobi: 9\n",
            "kevin,kevin,dia: 10\n",
            "kevin,kevin,futsal: 11\n",
            "kevin,kevin,suka: 12\n",
            "kevin,kevin,makan: 13\n",
            "kevin,kevin,mie: 14\n",
            "kevin,kevin,minum: 15\n",
            "kevin,kevin,matcha: 16\n",
            "kevin,lius,kevin: 17\n",
            "kevin,lius,lius: 18\n",
            "kevin,lius,bong: 19\n",
            "...\n",
            "\n",
            "===== The transition matrix is (Shape of trasition matrix: (4913, 4913)): =====\n",
            "[[0.05882353 0.05882353 0.05882353 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.05882353 0.05882353 0.05882353]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the transition matrix and unique states for obsevation\n",
        "print(\"===== The indices for unique states are: =====\")\n",
        "unique_words_count = len(unique_words)\n",
        "for word_name in list(product(*[unique_words for _ in range(n-1)]))[:20]:\n",
        "    print(f\"{','.join(word_name):10s}: {words_to_index(word_name, unique_words_count, unique_words)}\")\n",
        "print(\"...\", end=\"\\n\\n\")\n",
        "\n",
        "print(\"===== The transition matrix is (Shape of trasition matrix: {}): =====\".format(transition_matrix.shape))\n",
        "print(transition_matrix)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rnbUL1itveev",
        "outputId": "a16588a9-503f-4829-a439-ff70bce30094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1 (length=15): kevin lius bong icho 3x winner chemical engineering nus hobi dia futsal dia suka makan mie dia suka\n",
            "#2 (length=20): hobi dia futsal dia suka makan mie dia suka minum matcha hobi winner futsal bong futsal mie 3x suka bong suka nus kevin\n",
            "#3 (length=13): chemical engineering nus hobi dia futsal dia suka makan mie dia suka minum matcha makan minum\n"
          ]
        }
      ],
      "source": [
        "# [TODO] Write down 3 or more initial words and length of generated text to start the text generation\n",
        "experiments = [\n",
        "    # Each entry is (Initial words, the length of the generated text)\n",
        "   # ('i love', 10), ('cat love', 15), ('love me', 30)\n",
        "    ('kevin lius bong', 15), ('hobi dia futsal', 20), ('chemical engineering nus', 13)\n",
        "]\n",
        "\n",
        "for idx, (start_from, length) in enumerate(experiments, 1):\n",
        "    start_from = start_from.split(\" \")\n",
        "\n",
        "    # Generate text using the transition matrix\n",
        "    generated_text = generate(unique_words, transition_matrix, start_from, n, length=length)\n",
        "\n",
        "    # Print out the generated text\n",
        "    print(\"#{} (length={}): {}\".format(idx, length, generated_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrK8ourT6aE2"
      },
      "source": [
        "### Answer the following questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrUZ91rpvcPt"
      },
      "source": [
        "* (a) Write a new text of at least 15 words as the input.\n",
        "    * kevin lius bong. icho 3x winner. chemical engineering nus. hobi dia futsal. dia suka makan mie. dia suka minum matcha.\n",
        "\n",
        "* (b) Run the program 3 times with different output length and different initial words. Show the outputs.\n",
        "    * #1 (length=15): kevin lius bong icho 3x winner chemical engineering nus hobi dia futsal dia suka makan mie dia suka\n",
        "    * #2 (length=20): hobi dia futsal dia suka makan mie dia suka minum matcha hobi winner futsal bong futsal mie 3x suka bong suka nus kevin\n",
        "    * #3 (length=13): chemical engineering nus hobi dia futsal dia suka makan mie dia suka minum matcha makan minum\n",
        "\n",
        "* (c) Try different N of N-grams. How the N influences the output sequence?\n",
        "    * By changing the N of N-grams, it influences the length of the context used to predict the next word. As It looks at (n-1) inputs to predict its next state.\n",
        "    * For example,\n",
        "      * 1st case: n=3, the inputs: ('kevin lius',5), ('matcha bong',3). The output will then be processed accordingly with no error, because the input is 2 which is exactly 3-1=2.\n",
        "      * 2nd case: n=4, the previous input cannot be reused because now the n value is 4 so that means the inputs should contain 3 words as 4-1=3. For it to run properly, the input must be modified to ('kevin lius bong',5), ('dia suka makan',6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ej2DuDTveev"
      },
      "source": [
        "## Problem 3\n",
        "The Stationary Distribution of a Markov chain is a distribution of probabilities that remains unchanged after a transition from one state to another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EGILVlKvcPu"
      },
      "source": [
        "* (a) Ask an LLM (Large Language Model), such as ChatGPT, what are the applications of stationary distribution of a Markov chain. You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs).\n",
        "    * I used Gemini AI to answer this question\n",
        "    * This is the answer given:\n",
        "\n",
        "      * Markov Chain Monte Carlo (MCMC):\n",
        "\n",
        "        * Sampling from Complex Distributions: MCMC methods use Markov chains to generate samples from complex probability distributions that are difficult to sample directly. The stationary distribution of the Markov chain is designed to match the target distribution, ensuring that the generated samples are representative of the desired distribution.\n",
        "        * Bayesian Inference: MCMC is widely used in Bayesian statistics to estimate parameters or posterior distributions. The stationary distribution of the MCMC algorithm provides the posterior distribution of interest.\n",
        "\n",
        "      * Network Analysis:\n",
        "\n",
        "        * PageRank Algorithm: Google's PageRank algorithm, used to rank web pages, is based on a Markov chain model. The stationary distribution of the chain represents the long-term probability of a random surfer ending up on a particular page, which is used to determine the page's importance.\n",
        "        * Traffic Flow Analysis: Markov chains can be used to model traffic flow in networks, and the stationary distribution can provide insights into the long-term traffic patterns and congestion levels.\n",
        "\n",
        "      * Biology and Genetics:\n",
        "\n",
        "        * Genetic Drift: Markov chains are used to model genetic drift, the random fluctuations in allele frequencies in a population. The stationary distribution can be used to study the long-term effects of genetic drift on genetic diversity.\n",
        "        * Phylogenetics: Markov chains are used to reconstruct evolutionary relationships between species (phylogenies). The stationary distribution of the Markov chain representing the evolutionary process is used to infer the most likely phylogeny.\n",
        "\n",
        "      * Finance and Economics:\n",
        "\n",
        "        * Financial Modeling: Markov chains are used to model the evolution of financial variables, such as stock prices or interest rates. The stationary distribution can provide insights into the long-term expected values and variability of these variables.\n",
        "        * Credit Risk Assessment: Markov chains can be used to model the creditworthiness of borrowers. The stationary distribution can be used to assess the long-term probability of default and to develop risk management strategies.\n",
        "  * The prompt used: What are the applications of stationary distribution of a Markov chain\n",
        "  * I verify the results by cross-checking with the verified resources they enlist with the answer. In my case, i put the prompt on the google search bar and the list of references used was listed.\n",
        "\n",
        "* (b) Ask an LLM, such as ChatGPT, what numerical method is the most efficient approach to compute the stationary distribution? You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs)\n",
        "    * I used Gemini AI to answer this question\n",
        "    * This is the answer given:\n",
        "    * The most efficient numerical method to compute the stationary distribution of a Markov chain depends on the specific characteristics of the chain, such as its size, structure, and the desired level of accuracy. However, here are some commonly used methods:\n",
        "\n",
        "      1. Power Method:\n",
        "\n",
        "        This is a simple iterative method that involves repeatedly multiplying the initial probability distribution by the transition probability matrix.\n",
        "        It converges to the stationary distribution as the number of iterations increases.\n",
        "        The power method is generally efficient for small to medium-sized Markov chains.\n",
        "      2. Jacobi or Gauss-Seidel Iteration:\n",
        "\n",
        "        These methods solve the system of linear equations that define the stationary distribution.\n",
        "        They are often more efficient than the power method for larger Markov chains.\n",
        "        Jacobi iteration is a parallel algorithm, while Gauss-Seidel iteration is sequential.\n",
        "      3. Successive Over-Relaxation (SOR):\n",
        "\n",
        "        This is a variant of Gauss-Seidel iteration that can accelerate convergence by introducing a relaxation parameter.\n",
        "        The optimal relaxation parameter can significantly improve the efficiency of SOR.\n",
        "      4. Krylov Subspace Methods:\n",
        "\n",
        "        These methods are based on projecting the problem onto a smaller subspace.\n",
        "        They can be very efficient for large, sparse Markov chains.\n",
        "        Examples of Krylov subspace methods include Arnoldi iteration and GMRES.\n",
        "      5. Direct Methods:\n",
        "\n",
        "        For small Markov chains, direct methods like Gaussian elimination can be used to solve the system of linear equations.\n",
        "        However, direct methods can become computationally expensive for larger chains.\n",
        "        The choice of method depends on factors such as the size of the Markov chain, the sparsity of the transition matrix, and the desired level of accuracy. For small to medium-sized Markov chains, the power method or Jacobi/Gauss-Seidel iteration are often sufficient. For larger, sparse chains, Krylov subspace methods can be more efficient. It's also important to consider the specific characteristics of the Markov chain and the computational resources available when selecting a method.\n",
        "    * The prompt used: What numerical method is the most efficient approach to compute the stationary distribution?\n",
        "    * I verify the results by cross-checking with the verified resources they enlist with the answer. In my case, i put the prompt on the google search bar and the list of references used was listed.\n",
        "* (c) Implement the method suggested by the LLM. Use the transition matrix generated in question 2.a as an input to compute its stationary distribution.\n",
        "    * Please implememt the method `compute_stationary_distribution` below\n",
        "    * I used Chat GPT\n",
        "* (d) The theory of probability matrix is given in the textbook 6.8, Eigenvalues/eigenvector of nonegtive matrices. Read textbook 6.8 and ask two questions that you are curious most about it\n",
        "    * What are the real-life applications of Frobeinus Theorem? Is it more commonly applied then Perron's theorem?\n",
        "    * What are the real-life applications of Perron's Theorem? Is it more commonly applied then Frobeinus theorem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-VwWF0P1veev"
      },
      "outputs": [],
      "source": [
        "def compute_stationary_distribution(transition_matrix: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute the stationary distribution of a Markov chain.\n",
        "\n",
        "    Parameters:\n",
        "        transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n",
        "    Returns:\n",
        "        The stationary distribution of the Markov chain.\n",
        "    \"\"\"\n",
        "     # [TODO] Implement the function to compute the stationary distribution of a Markov chain, using the method suggested by the LLM\n",
        "    # Ensure the transition matrix is a valid probability matrix\n",
        "    assert np.allclose(transition_matrix.sum(axis=1), 1), \"Rows of transition matrix must sum to 1\"\n",
        "\n",
        "    # Number of states\n",
        "    n = transition_matrix.shape[0]\n",
        "\n",
        "    # Solve the equation (P^T - I)v = 0\n",
        "    A = np.transpose(transition_matrix) - np.eye(n)\n",
        "\n",
        "    # Append a constraint to ensure the probabilities sum to 1\n",
        "    A = np.vstack([A, np.ones(n)])\n",
        "    b = np.zeros(n + 1)\n",
        "    b[-1] = 1  # Constraint for the sum of probabilities\n",
        "\n",
        "    # Use NumPy's least squares to find the stationary distribution\n",
        "    stationary_distribution, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n",
        "\n",
        "    # Normalize to ensure it's a probability distribution\n",
        "    stationary_distribution /= stationary_distribution.sum()\n",
        "\n",
        "    return stationary_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-owTykzFveev",
        "outputId": "02d65124-0588-4177-cd70-6e66e415c5a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0001978 0.0001978 0.0001978 ... 0.0002086 0.0002086 0.0002086]\n"
          ]
        }
      ],
      "source": [
        "# Compute the stationary distribution for the transition matrix obtained from the previous problem\n",
        "stationary_distribution = compute_stationary_distribution(transition_matrix)\n",
        "\n",
        "# Convert the stationary distribution to float16 to prevent something like -1.2345678e-16 but it is actually 0\n",
        "print(stationary_distribution.astype(np.float16))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}